{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import eval_functions as evals\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data = []\n",
    "with open('all_models_pred_small_dev.json') as f:\n",
    "    scores_data = json.load(f)\n",
    "    \n",
    "multi_dataset = []\n",
    "with open('multi_dataset.json') as f:\n",
    "    multi_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(sorted_i_j, tokens, k, space_token):\n",
    "    \"\"\"\n",
    "    params: sorted_i_j (dict of (i, j) index pairs and their summed score (from get_score_dict))\n",
    "            tokens (list of tokens from question and passage)\n",
    "            k (number of answers to be returned (num of true_answers))\n",
    "    returns: predicted answers from the model\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    answers_indices = [] # i, j pairs of answers\n",
    "    answers = [] # string answers \n",
    "    while len(answers) < k and counter < len(sorted_i_j):\n",
    "        pair = sorted_i_j[counter]\n",
    "        if pair[1] >= pair[0] and pair[0] != 0: #end token after start token\n",
    "            if counter == 0: # first i,j\n",
    "                answer = tokens[pair[0]] #first token (i)\n",
    "                for i in range(pair[0] + 1, pair[1] + 1):\n",
    "                    if space_token == '##': #for bert\n",
    "                        if tokens[i][0:2] == space_token:\n",
    "                            answer += tokens[i]\n",
    "                        else: answer += ' ' + tokens[i]\n",
    "                    elif tokens[i][0:1] != space_token:\n",
    "                        answer += tokens[i]\n",
    "                    else:\n",
    "                        answer += ' ' + tokens[i]\n",
    "                answers_indices.append(sorted_i_j[counter])\n",
    "                answer = answer.replace(space_token, '')\n",
    "                answers.append(answer)\n",
    "            elif (pair[0] > sorted_i_j[counter-1][1]) or (sorted_i_j[counter-1][0] > pair[1]):\n",
    "                for old_pair in sorted_i_j[:counter]:\n",
    "                    if pair[0] not in range(old_pair[0], old_pair[1]):\n",
    "                        add_answer = True\n",
    "                    else:\n",
    "                        add_answer = False\n",
    "                        break\n",
    "                # ^start token of current is after end token of prev; end token of current is before start of prev\n",
    "                if add_answer == True:\n",
    "                    answer = tokens[pair[0]]\n",
    "                    for i in range(pair[0] + 1, pair[1] + 1):\n",
    "                        if space_token == '##':\n",
    "                            if tokens[i][0:2] == space_token:\n",
    "                                answer += tokens[i]\n",
    "                            else: answer += ' ' + tokens[i]\n",
    "                        elif tokens[i][0:1] != space_token:\n",
    "                            answer += tokens[i]\n",
    "                        else:\n",
    "                            answer += ' ' + tokens[i]\n",
    "                    answers_indices.append(sorted_i_j[counter])\n",
    "                    answer = answer.replace(space_token, '')\n",
    "                    answers.append(answer)\n",
    "        counter += 1\n",
    "    if len(answers_indices) < k:\n",
    "        while len(answers_indices) < k:\n",
    "            answers_indices.append((-1, -1))\n",
    "    for i in answers_indices:\n",
    "        if i == (-1, -1):\n",
    "            answers.append('')\n",
    "    return answers\n",
    "\n",
    "def get_score_dict(start_scores, end_scores):\n",
    "    '''    \n",
    "    params: start and end score arrays\n",
    "    returns: dictionary of (i, j) pairs sorted by descending order of sum of scores\n",
    "    i = start score index\n",
    "    j = end score index\n",
    "    score = sum of start_scores[i] and end_scores[j]\n",
    "    '''\n",
    "    start_scores_copy = start_scores.copy()\n",
    "    end_scores_copy = end_scores.copy()\n",
    "    i_j_scores = []\n",
    "    for x in range(len(start_scores)):\n",
    "        i = np.argmax(start_scores_copy)\n",
    "        start_scores_copy[i] = 0\n",
    "        j = np.argmax(end_scores_copy)\n",
    "        end_scores_copy[j] = 0\n",
    "        i_j_scores.append((i, j))\n",
    "    return i_j_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(true_answers, pred_answers):\n",
    "    count = 0\n",
    "    true_answers_lower = [answer.lower() for answer in true_answers]\n",
    "    pred_answers_lower = [answer.lower() for answer in pred_answers]\n",
    "    for answer in pred_answers_lower:\n",
    "        if answer in true_answers_lower:\n",
    "            count += 1\n",
    "        if answer == '' and 'noAnswer' in true_answers:\n",
    "            count += 1\n",
    "    em = count / len(true_answers)\n",
    "    return em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_pred_arrays(true_answers, pred_answers, passage, model_name):\n",
    "    \"\"\"\n",
    "    params: true_answers: list of true answers\n",
    "            pred_answers: list of predicted answers\n",
    "            passage: original passage string\n",
    "            model_name: 'bert-base', 'roberta-large', etc.\n",
    "    returns: true_array: array of 1s and 0s, same length as len(passage)\n",
    "                         1 = character is in a true answer\n",
    "                         0 = character is not in a true answer\n",
    "             pred_array: same as true_array, but for pred_answers\n",
    "    \"\"\"\n",
    "    true_array = np.zeros(len(passage))\n",
    "    for answer in true_answers:\n",
    "        true_array = set_ones(answer, passage, true_array, model_name)\n",
    "\n",
    "    pred_array = np.zeros(len(passage))\n",
    "    for answer in pred_answers:\n",
    "        if answer != '':\n",
    "            pred_array = set_ones(answer, passage, pred_array, model_name)\n",
    "    return true_array, pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ones(answer, passage, array, model_name):\n",
    "    \"\"\"\n",
    "    sets the ones (answer chars) in the array of zeros (non-answer chars)\n",
    "    \"\"\"\n",
    "    if ('( ' in answer or ' )' in answer) or (' ,' in answer or ' .' in answer):\n",
    "        answer = answer.replace('( ', '(')\n",
    "        answer = answer.replace(' )', ')')\n",
    "        answer = answer.replace(' ,', ',')\n",
    "        answer = answer.replace(' .', '.')\n",
    "        \n",
    "    if model_name == 'roberta-base' or model_name == 'roberta-large':\n",
    "        start_id = passage.find(answer)\n",
    "    else:\n",
    "        start_id = passage.lower().find(answer.lower())\n",
    "    \n",
    "    if start_id + len(answer) >= len(passage):\n",
    "        for index in range(start_id, start_id + len(answer) - 1):\n",
    "            array[index] = 1\n",
    "    else:\n",
    "        for index in range(start_id, start_id + len(answer)):\n",
    "            array[index] = 1\n",
    "        \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_bleu_score(true_answers, pred_answers, model_name):\n",
    "    \"\"\"\n",
    "    returns the blue score for each question set\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    references = []\n",
    "    if model_name == 'roberta-base' or model_name == 'roberta-large':\n",
    "        for true_answer in true_answers:\n",
    "            references.append(true_answer.split())\n",
    "    else:\n",
    "        for true_answer in true_answers:\n",
    "            references.append([word.lower() for word in true_answer.split()])\n",
    "    for pred_answer in pred_answers: # calc bleu for each pred_answer\n",
    "        hypothesis = pred_answer.split()\n",
    "        curr_score = nltk.bleu_score.sentence_bleu(references, hypothesis, weights=[1.])\n",
    "        total += curr_score\n",
    "\n",
    "    return total/len(pred_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base\n",
      "avg_em: 0.25\n",
      "avg_f1_micro: 0.4752475247524752\n",
      "avg_f1_c1: 0.47342342342342336\n",
      "avg_prec_micro: 0.7741935483870968\n",
      "avg_prec_c1: 0.8205128205128206\n",
      "avg_recall_micro: 0.34285714285714286\n",
      "avg_recall_c1: 0.39439534231200896\n",
      "avg_bleu: 0.3333333333333333\n",
      " \n"
     ]
    }
   ],
   "source": [
    "model_names = [('twmkn9/bert-base-uncased-squad2', 'bert-base')]\n",
    "\"\"\", ('deepset/bert-large-uncased-whole-word-masking-squad2', 'bert-large'), \n",
    "               ('deepset/roberta-base-squad2', 'roberta-base'), (\"ahotrod/roberta_large_squad2\", 'roberta-large'), \n",
    "               ('twmkn9/albert-base-v2-squad2', 'albert-base'), ('ktrapeznikov/albert-xlarge-v2-squad-v2', 'albert-xlarge')]\n",
    "\"\"\"\n",
    "               \n",
    "for model_name in model_names:\n",
    "    print(model_name[1])\n",
    "    #get tokenizers\n",
    "    if model_name[0] == 'deepset/roberta-base-squad2':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        space_token = 'Ġ'\n",
    "    elif model_name[0] == \"ahotrod/roberta_large_squad2\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "        space_token = 'Ġ'\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name[0])\n",
    "        if model_name[1] == 'bert-base' or model_name[1] == 'bert-large':\n",
    "            space_token = '##'\n",
    "        else:\n",
    "            space_token = '▁'\n",
    "            \n",
    "    # reset averages\n",
    "    avg_exact_match, avg_f1_micro, avg_f1_c1, avg_prec_micro, avg_prec_c1, avg_recall_micro, avg_recall_c1, avg_bleu = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    tn_sum, fp_sum, fn_sum, tp_sum = 0, 0, 0, 0\n",
    "    \n",
    "    # get answers\n",
    "    for score_set in scores_data[model_name[1]]: # each model\n",
    "        # replace w multi\n",
    "        for triplet in multi_dataset['multi_dataset']: # match to original dataset\n",
    "            if triplet['question_id'] == score_set['question_id']:\n",
    "                question = triplet['question']\n",
    "                passage = triplet['passage']\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenizer(question, passage)['input_ids'])\n",
    "        sorted_i_j = get_score_dict(score_set['start_scores'], score_set['end_scores'])\n",
    "        true_answers = score_set['true_answers']\n",
    "        pred_answers = get_answers(sorted_i_j, tokens, len(score_set['true_answers']), space_token)\n",
    "\n",
    "        # get metrics\n",
    "        true_array, pred_array = get_true_pred_arrays(true_answers, pred_answers, passage, model_name[1])\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(true_array, pred_array).ravel()\n",
    "        \n",
    "        e_m = exact_match(true_answers, pred_answers)\n",
    "        avg_exact_match += e_m\n",
    "        \n",
    "        tn_sum += tn\n",
    "        fp_sum += fp\n",
    "        fn_sum += fn\n",
    "        tp_sum += tp\n",
    "\n",
    "        prec_c1 = precision_score(true_array, pred_array, average='binary', pos_label=1.)\n",
    "        avg_prec_c1 += prec_c1\n",
    "        \n",
    "        recall_c1 = recall_score(true_array, pred_array, average='binary', pos_label=1.)\n",
    "        avg_recall_c1 += recall_c1\n",
    "        \n",
    "        f1_c1 = f1_score(true_array, pred_array, average='binary', pos_label=1.)\n",
    "        avg_f1_c1 += f1_c1\n",
    "\n",
    "        bleu = avg_bleu_score(true_answers, pred_answers, model_name[1])\n",
    "        avg_bleu += bleu\n",
    "        \n",
    "    avg_prec_micro = tp_sum / (tp_sum + fp_sum)\n",
    "    avg_recall_micro = tp_sum / (tp_sum + fn_sum)\n",
    "    avg_f1_micro = 2 * ((avg_prec_micro*avg_recall_micro) / (avg_prec_micro + avg_recall_micro))\n",
    "            \n",
    "    print('avg_em: {}'.format(avg_exact_match/len(scores_data[model_name[1]])))\n",
    "    print('avg_f1_micro: {}'.format(avg_f1_micro))\n",
    "    print('avg_f1_c1: {}'.format(avg_f1_c1/len(scores_data[model_name[1]])))\n",
    "    print('avg_prec_micro: {}'.format(avg_prec_micro))\n",
    "    print('avg_prec_c1: {}'.format(avg_prec_c1/len(scores_data[model_name[1]])))\n",
    "    print('avg_recall_micro: {}'.format(avg_recall_micro))\n",
    "    print('avg_recall_c1: {}'.format(avg_recall_c1/len(scores_data[model_name[1]])))\n",
    "    print('avg_bleu: {}'.format(avg_bleu/len(scores_data[model_name[1]])))\n",
    "    print(' ')\n",
    "\n",
    "\n",
    "# metrics_df = pd.DataFrame.from_dict(metrics_data, orient='index')\n",
    "# display(metrics_df)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>row_1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_2</th>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1  3  4  5\n",
       "row_1  3  2  1  0\n",
       "row_2  a  b  c  d"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns=[1, 3, 4, 5])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "{} &  0 &  1 &  2 &  3 \\\\\n",
      "\\midrule\n",
      "row\\_1 &  3 &  2 &  1 &  0 \\\\\n",
      "row\\_2 &  a &  b &  c &  d \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model --> metric --> answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
